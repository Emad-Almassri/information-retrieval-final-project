{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from geopy.geocoders import Nominatim\n",
    "import spacy\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SS\\AppData\\Local\\Temp\\ipykernel_1732\\491064492.py:1: DeprecationWarning: The 'http_auth' parameter is deprecated. Use 'basic_auth' or 'bearer_auth' parameters instead\n",
      "  es = Elasticsearch(\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\n",
    "    [{'host': 'localhost', 'port': 9200, 'scheme': 'http'}],\n",
    "    http_auth=('emad2', 'emadmassri'))\n",
    "\n",
    "index_name = \"news_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SS\\AppData\\Local\\Temp\\ipykernel_1732\\2178478941.py:65: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.create(index=index_name, ignore=400, body=configurations)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'news_index'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "configurations ={\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"Title\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"autocomplete\": {\n",
    "            \"type\": \"search_as_you_type\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"Content\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"custom_content_analyzer\"\n",
    "      },\n",
    "      \"Authors\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"first_name\": { \"type\": \"keyword\" },\n",
    "          \"last_name\": { \"type\": \"keyword\" },\n",
    "          \"email\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"Date\": {\n",
    "        \"type\": \"date\"\n",
    "      },\n",
    "      \"Geopoint\": {\n",
    "        \"type\": \"geo_point\"\n",
    "      },\n",
    "      \"TemporalExpressions\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"Georeferences\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"custom_content_analyzer\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\", \"stemmer_filter\"]\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"stemmer_filter\": {\n",
    "          \"type\": \"stemmer\",\n",
    "          \"name\": \"english\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# create index\n",
    "es.indices.create(index=index_name, ignore=400, body=configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "geolocator = Nominatim(user_agent=\"geo_app\", timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text by convert to lowercase and remove stop words and tokens with length < 3, and perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(content):\n",
    "    clean_content = re.sub(r'<[^>]+>', '', content)\n",
    "    \n",
    "    tokens = word_tokenize(clean_content.lower())  \n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_text = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) >= 3]\n",
    "    processed_paragraph = ' '.join(processed_text)\n",
    "    return processed_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author_info(author_tag):\n",
    "    authors_info = []\n",
    "    if author_tag:\n",
    "        author_text = author_tag.get_text().strip().replace('by ', '')\n",
    "        authors = author_text.split(',')\n",
    "        for author in authors:\n",
    "            parts = author.strip().split()\n",
    "            if len(parts) >= 1:\n",
    "                first_name = parts[0]\n",
    "                last_name = ' '.join(parts[1:])\n",
    "                author_info = {\n",
    "                    \"first_name\": first_name.strip(),\n",
    "                    \"last_name\": last_name.strip()if last_name else None,\n",
    "                    \"email\": None  \n",
    "                }\n",
    "                authors_info.append(author_info)\n",
    "    return authors_info if authors_info else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date_tags):\n",
    "    date_obj = None\n",
    "    \n",
    "    if date_tags and isinstance(date_tags, list): \n",
    "        date_tag = date_tags[0]\n",
    "        date_str = date_tag.text.strip()\n",
    "\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, \"%d-%b-%Y %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            print(f\"Error: Unable to parse date string '{date_str}'\")\n",
    "    \n",
    "    return date_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reuters_tags(soup):\n",
    "    for reuters_tag in soup.find_all('reuters'):\n",
    "        \n",
    "        date = convert_date([reuters_tag.find('date')]) if reuters_tag.find('date') else None\n",
    "        \n",
    "        \n",
    "        topics = [topic.get_text() for topic in reuters_tag.find('topics').find_all('d')] if reuters_tag.find('topics') else None\n",
    "        \n",
    "        \n",
    "        places = [place.get_text() for place in reuters_tag.find('places').find_all('d')] if reuters_tag.find('places') else None\n",
    "        \n",
    "        \n",
    "        title = reuters_tag.find('title').get_text() if reuters_tag.find('title') else None\n",
    "        \n",
    "        \n",
    "        author_tag = reuters_tag.find('author')\n",
    "        author_info = extract_author_info(author_tag) if author_tag else None\n",
    "        \n",
    "        \n",
    "        content = reuters_tag.find('text').get_text() if reuters_tag.find('text') else None\n",
    "        \n",
    "        \n",
    "        content = clean_content(content) if content else None\n",
    "        \n",
    "        georeferences = []\n",
    "        coordinates = []\n",
    "        \n",
    "        \n",
    "        for place_name in places or []:\n",
    "            location = geolocator.geocode(place_name)\n",
    "            if location:\n",
    "                georeferences.append(place_name)\n",
    "                coordinates.append({'latitude': location.latitude, 'longitude': location.longitude})\n",
    "        \n",
    "        temporal_expressions = []\n",
    "        if content:\n",
    "            doc = nlp(content)\n",
    "            temporal_expressions = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        document_dict = {\n",
    "            \"Topics\": topics,\n",
    "            \"Title\": title,\n",
    "            \"Content\": content,\n",
    "            \"Authors\": author_info,\n",
    "            \"Date\": date,\n",
    "            \"Georeferences\": georeferences,\n",
    "            \"Coordinates\": {\"latitude\": coordinates[0][\"latitude\"], \"longitude\": coordinates[0][\"longitude\"]},\n",
    "            \"TemporalExpressions\": temporal_expressions,\n",
    "        }\n",
    "\n",
    "        \n",
    "        es.index(index=index_name, body=document_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('./s'):\n",
    "    if filename.endswith('.sgm'):\n",
    "        file_path = os.path.join('./s', filename)\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            sgm_content = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(sgm_content, 'html.parser')\n",
    "\n",
    "        process_reuters_tags(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
